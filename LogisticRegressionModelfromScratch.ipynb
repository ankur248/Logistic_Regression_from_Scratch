{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class logistic_classifier(object):\n",
    "    def __init__(self, coeff):\n",
    "        # coeff is the regularization coefficient\n",
    "        self.coeff = coeff\n",
    "        # w will be our linear separator\n",
    "        self.w = 0.0\n",
    "\n",
    "\n",
    "    def compute_probabilities(self, Xtr):\n",
    "        # function to compute mu_i (sigmoid(wTx)).\n",
    "        # Complete this function. \n",
    "      \n",
    "        probabilities = self.sigmoid(np.dot(Xtr,self.w))\n",
    "     \n",
    "        return probabilities\n",
    "        \n",
    "    def compute_loss(self, probabilities, Ytr):\n",
    "        # function to compute training loss.\n",
    "        # Keep in mind that log(x) tends to infinity if x is too close\n",
    "        # to zero. Thus, whenever you think x could be close to\n",
    "        # zero, add a small positive value (like 10^(-6)) to x so that\n",
    "        # log doesn't return too big a value. \n",
    "        \n",
    "        Cost= (np.dot((Ytr),(np.log(probabilities))) + np.dot((1-Ytr),(np.log(1-probabilities))))*(-1/len(Ytr))\n",
    "        \n",
    "        return Cost\n",
    "        \n",
    "    def compute_gradients(self, probabilities, Ytr, Xtr):\n",
    "        # function to compute gradients with respect to w.\n",
    "        # Complete it, keeping in mind the two gradient components -\n",
    "        # the one for the logistic loss and the one for the regularizer.\n",
    "        #gradient=np.matmul(np.transpose(probabilities-Ytr), Xtr)\n",
    "        gradient=np.dot((probabilities-Ytr), Xtr)\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self, learning_rate, grads):\n",
    "        # function to update weights. This needs to be filled in. \n",
    "        self.w-=learning_rate * grads\n",
    "        \n",
    "\n",
    "    def sigmoid(self, inputs):\n",
    "        # function to compute sigmoid of the inputs.\n",
    "        # Keep in mind that if you exponentiate too large or too small\n",
    "        # a value, the result might be out of bounds of computer\n",
    "        # precision. Thus, put a lower and an upper cap on the\n",
    "        # input to the exponential function. \n",
    "        IP=np.exp(-inputs)\n",
    "        exp=1/(1 + IP)\n",
    "        exp[exp>0.999999]=0.999999\n",
    "        exp[exp<0.000001]=0.000001\n",
    "        return exp\n",
    "       \n",
    "        #np.clip(ghj,-999999,999999,out=ghj)\n",
    "        #return 1 / (1 + ghj) \n",
    "        \n",
    "    def fit(self, Xtr, Ytr):\n",
    "        '''\n",
    "        This function trains the logistic regression model on the \n",
    "        given training data\n",
    "        '''\n",
    "        # num_iters: number of iterations that gradient descent\n",
    "        # should run for.\n",
    "        learning_rate = 0.00005\n",
    "        \n",
    "        self.num_iters = 10000\n",
    "\n",
    "        # random initialization for w. \n",
    "        self.w = np.random.normal(0.0, 0.1, Xtr.shape[1])\n",
    "        \n",
    "        for iter in range(self.num_iters):\n",
    "            probabilities = self.compute_probabilities(Xtr)\n",
    "        \n",
    "            train_loss = self.compute_loss(probabilities, Ytr)\n",
    "            if iter % 1000 == 0:\n",
    "                print ( \"Train Loss = \"+str(train_loss))\n",
    "            \n",
    "            grads = self.compute_gradients(probabilities, Ytr, Xtr)\n",
    "            grads = grads / Xtr.shape[0]\n",
    "            self.update_weights(learning_rate, grads)\n",
    "\n",
    "    def predict(self, Xts):\n",
    "        '''\n",
    "        This function gives label predictions on the dataset fed to it. \n",
    "        '''\n",
    "        linear_combinations = np.matmul(Xts, self.w)\n",
    "        probabilities = self.sigmoid(linear_combinations)\n",
    "        self.predictions = np.zeros(probabilities.shape)\n",
    "        self.predictions = (probabilities > 0.5)\n",
    "        return self.predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57) (921, 57) (3680,) (921,)\n",
      "Train Loss = 5.24413755475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda4\\lib\\site-packages\\ipykernel\\__main__.py:47: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.71769294472\n",
      "Train Loss = 0.696251567036\n",
      "Train Loss = 0.678333199709\n",
      "Train Loss = 0.663108340585\n",
      "Train Loss = 0.649993108332\n",
      "Train Loss = 0.638373848918\n",
      "Train Loss = 0.628254965633\n",
      "Train Loss = 0.619663253426\n",
      "Train Loss = 0.612072219904\n",
      "69.38110749185668\n"
     ]
    }
   ],
   "source": [
    "dataset = \"spam\"\n",
    "# adjust the path according to where you have stored the dataset. \n",
    "path = \"C:/Users/Dell/Desktop/Spam Data/\" \n",
    "\n",
    "Xtr = np.load(path + \"Xtrain.npy\")\n",
    "Ytr = np.load(path + \"Ytrain.npy\")\n",
    "Xts = np.load(path + \"Xtest.npy\")\n",
    "Yts = np.load(path + \"Ytest.npy\")\n",
    "\n",
    "print (Xtr.shape, Xts.shape,Ytr.shape, Yts.shape)\n",
    "\n",
    "model = logistic_classifier(coeff=0.0)\n",
    "model.fit(Xtr, Ytr)\n",
    "predictions = model.predict(Xts)\n",
    "\n",
    "accuracy = 0.0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == Yts[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(predictions)\n",
    "accuracy *= 100\n",
    "test_accuracy = accuracy\n",
    "print (test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
